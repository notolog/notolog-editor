# Latin lexemes settings_dialog.py
lexemes = {
    "tab_module_llama_cpp_config": "Modulus llama.cpp",

    "module_llama_cpp_config_label": "Modulus llama.cpp",
    "module_llama_cpp_config_path_label": "Locatio Modeli",
    "module_llama_cpp_config_path_input_placeholder_text": "Elige aut inscribe viam modeli",
    "module_llama_cpp_config_path_input_accessible_description":
        "Campus input cum selectorio ad viam modeli localis specificandam. Sustinet modela in forma GGUF,\n"
        "formatum file binarium optimatum ad reponendum modela usui cum GGML et executoribus GGML-fundatis.",
    "module_llama_cpp_config_path_input_filter_text": "GGUF Files",

    "module_llama_cpp_config_context_window_label": "Magnitudo Fenestrae Contextus",
    "module_llama_cpp_config_context_window_input_accessible_description":
        "Statuit numerum signorum quae model considerat ad responsa generanda. Moderatur quantum contextus prioris utitur.",

    "module_llama_cpp_chat_formats_label": "Formae Colloquii",
    "module_llama_cpp_chat_formats_combo_placeholder_text": "Elige formam colloquii",
    "module_llama_cpp_chat_formats_combo_accessible_description":
        "Menu dropdown ad formam eligendam quae ad colloquia modeli utitur.",

    "module_llama_cpp_config_system_prompt_label": "Promptum Systematis",
    "module_llama_cpp_config_system_prompt_edit_placeholder_text": "Inscribe textum prompti systematis",
    "module_llama_cpp_config_system_prompt_edit_accessible_description":
        "Campus texti ad prompta systematis ingredienda quae responsa modeli dirigunt.",

    "module_llama_cpp_config_response_temperature_label": "Temperatura Responsi: {temperature}",
    "module_llama_cpp_config_response_temperature_input_accessible_description":
        "Adiustat casualitatem responsorum modeli. Maiores valores producunt outputs variabilius,\n"
        "dum valores inferiores resultunt in responsis praedicibilioribus.",

    "module_llama_cpp_config_response_max_tokens_label": "Maxima Signorum per Responsionem",
    "module_llama_cpp_config_response_max_tokens_input_accessible_description":
        "Limitat numerum signorum in responsis modeli ad limitem fenestrae contextus actualis.\n"
        "Valor nihili capacitas fenestrae contextus assumit.",

    "module_llama_cpp_config_prompt_history_size_label": "Magnitudo Historiae Promptorum",
    "module_llama_cpp_config_prompt_history_size_input_accessible_description":
        "Moderatur numerum invectionum in historia promptorum quae a systemate retentae sunt ad referentiam.\n"
        "Valor nihili invectiones illimitatas permittit."
}
